{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d8ab9c",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "## Аналитическое решение задачи о регрессии\n",
    "Аналитическая формула для оценки параметров линейной регрессии методом наименьших квадратов:\n",
    "\n",
    "$$\n",
    "\\hat{w} = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $ X $ — матрица признаков (с добавленным столбцом единиц для смещения),\n",
    "- $ y $ — вектор целевой переменной,\n",
    "- $ \\hat{w} $ — вектор оптимальных коэффициентов.\n",
    "\n",
    "Результат - вектор коэффициентов, который наилучшим образом аппроксимирует зависимость между X и Y в смысле минимализации суммы квадратов ошибок. Этот метод известен как метод наименьших квадратов.\n",
    "## Регуляризация\n",
    "Регуляризация — это техника, используемая в машинном обучении для предотвращения переобучения модели. В частности, регуляризация L1 и L2 помогают управлять сложностью модели и улучшать её обобщающую способность путем добавления штрафов к параметрам модели.\n",
    "Обобщённая задача минимизации с регуляризацией выглядит так:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\sum_{i=1}^{N} \\left( y_i - f(x_i, w) \\right)^2 + \\lambda R(w)\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $ R(w) $ — регуляризатор (например, L1 или L2-норма),\n",
    "- $ \\lambda $ — коэффициент регуляризации, контролирующий баланс между ошибкой и сложностью модели.\n",
    "\n",
    "L1-регуляризация (Lasso):\n",
    "\n",
    "$$\n",
    "R(w) = \\|w\\|_1 = \\sum_{j=1}^{d} |w_j|\n",
    "$$\n",
    "\n",
    "L2-регуляризация (Ridge):\n",
    "\n",
    "$$\n",
    "R(w) = \\|w\\|_2^2 = \\sum_{j=1}^{d} w_j^2\n",
    "$$\n",
    "\n",
    "Сравнение L1 и L2\n",
    "\n",
    " - L2 регуляризация делает все коэффициенты небольшими, но ненулевыми, что полезно для стабилизации модели и борьбы с мультиколлинеарностью.\n",
    " - L1 регуляризация устанавливает некоторые коэффициенты в ноль, что делает её эффективным инструментом для выбора признаков и работы с разреженными моделями.\n",
    " \n",
    "Выбор между L1 и L2 зависит от конкретной задачи. Если целью является выбор небольшого числа значимых признаков, предпочтительнее использовать L1. Если важно сохранить все признаки, но контролировать их влияние, лучше подойдет L2.\n",
    "## Использование линейных моделей для подбора нелинейных зависимостей\n",
    "Чтобы применить линейные методы (например, линейную регрессию, Ridge, Lasso) для моделирования нелинейных зависимостей, нужно преобразовать исходные данные в новые признаки, которые будут отражать нелинейность. Например, можно добавить полиномиальные члены, взаимодействия признаков или другие нелинейные преобразования.\n",
    "Допусти у нас есть два признака X1 и X2. Мы можем создать новые признаки следующим образом:\n",
    "\n",
    "- Полиномиальные термины x1^2, x2^2, x1, x2\n",
    "- Логарифмические преобразования: log(x1)\n",
    "- Другие нелинейные функции: sin(x), cos(x)\n",
    "Затем мы применяем линейные модели к новым признакам, что позволяет учесть нелинейные зависимости между исходными признаками и целевой переменной.\n",
    "\n",
    "Этот подход называется **полиномиальной регрессией**, и он широко применяется в машинном обучении для построения сложных моделей на основе простых линейных методов."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
